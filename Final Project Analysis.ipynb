{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods & Results: Loading in Libraries & Reading in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, before we begin coding, we must load the required libaries for R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(repr)\n",
    "library(tidymodels)\n",
    "library(cowplot)\n",
    "library(digest)\n",
    "library(repr)\n",
    "library(GGally)\n",
    "library(ISLR)\n",
    "options(repr.matrix.max.rows = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will begin reading the file into JupyterHub. Since the file is retrieved from an outside URL, we will download the file to JupyterHub, and since the cells are seperated by semi-colons, instead of normally reading the file through read_csv, we will read the file by read_csv2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the data\n",
    "temp <- tempfile()\n",
    "download.file(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00368/Facebook_metrics.zip\",temp)\n",
    "\n",
    "#Reading the data in R\n",
    "facebook <- read_csv2(unz(temp, \"dataset_Facebook.csv\"))\n",
    "head(facebook)\n",
    "unlink(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1: Initial Dataset of Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our dataset, it passes the test for tidy data (each row is a single observation, each column is a single variable, each value is a single cell), so no further cleaning data tools are required. However, the variable names are separated by white spaces instead of underscores (_). To deal with this issue, we will rename our predictor columns to avoid additional headaches along the way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming \"Post Hour\"\n",
    "facebook_1 <- facebook %>% rename(post_hour = `Post Hour`) %>% \n",
    "    rename(post_day = `Post Weekday`) %>% \n",
    "    rename(total_interactions = `Total Interactions`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods & Results: Exploratory Data Analysis and Splitting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of our exploratory data analysis, we'll take a quick overview of our predictor variable, plotting \"Total Interactions\" against a time variable (In this case, we'll use Post Hour) through a scatterplot, just to filter out any potential outliers in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploratory_scatter_plot <- ggplot(facebook_1, aes(x = post_hour, y = total_interactions)) +\n",
    "                            geom_point() +\n",
    "                            labs(x = \"Post Hour (Hours)\", y = \"Total Interactions\", title = \"Total Interactions vs. Post Hour\") \n",
    "exploratory_scatter_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2: Exploratory Scatter Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm. It seems here that there is one post that has more than six thousand total interactions, with only around ten posts that have more than a thousand total interactions. These outliers can cause implications as it can skew the results of our data (large standard deviations) and make our graph look silly, like in the example above. Therefore, let's filter our total interactions to be less than a thousand to retrieve the most optimal results when we perform our regression models. And as stated in our introduction, let's also create a new variable for the hour of the week through the mutate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(2021)\n",
    "facebook_filtered_interactions <- facebook_1 %>% filter(total_interactions < 1000) %>% \n",
    "    mutate(hour_of_week = (post_day - 1) * 24 + post_hour)\n",
    "facebook_filtered_interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure  3: Filtering of Total Interactions and Addition of Post_Week Variable Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check that we have included our hour_of_week variable properly into the dataset, and sure enough, it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's build our exploratory table of our training dataset that we will be performing our K-Nearest Neighbors Regression on, which includes the number of rows, the number of columns and the number of rows with missing data values. However, to find to the means to our predictor variables (likes, comments, shares, and our total interactions), we will also need to filter out the number of rows with missing data values, so that we would not encounter any problems (such as a \"NA\" result) when finding the average. Altogether, we will mutate each of the results we found into one big table in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(2021)\n",
    "\n",
    "#Filtering missing data\n",
    "facebook_filtered_na <- facebook_filtered_interactions %>%\n",
    "                            filter(!is.na(Paid)) %>% \n",
    "                            filter(!is.na(comment)) %>% \n",
    "                            filter(!is.na(like)) %>% \n",
    "                            filter(!is.na(share)) %>%                          \n",
    "                            filter(!is.na(total_interactions))\n",
    "\n",
    "#Missing Rows of Data\n",
    "missing_rows <- sum(colSums(is.na(facebook)))\n",
    "\n",
    "#Average of Each Predictor Variable\n",
    "average_likes <- mean(facebook_filtered_na[[\"like\"]])\n",
    "average_comments <- mean(facebook_filtered_na[[\"comment\"]])\n",
    "average_shares <- mean(facebook_filtered_na[[\"share\"]])\n",
    "average_total_interactions <- mean(facebook_filtered_na[[\"total_interactions\"]])\n",
    "\n",
    "#Total Number of Observations\n",
    "observation_total <- nrow(facebook_filtered_na)\n",
    "\n",
    "#Total Number of Variables\n",
    "variable_total <- ncol(facebook_filtered_na)\n",
    "\n",
    "#Exploratory Data Analysis Table\n",
    "exploratory_table <- facebook_filtered_na %>%\n",
    "                     mutate(facebook_filtered_na, observation_total = observation_total) %>%\n",
    "                     mutate(facebook_filtered_na, variable_total = variable_total) %>%\n",
    "                     mutate(facebook_filtered_na, average_likes = average_likes) %>%\n",
    "                     mutate(facebook_filtered_na, average_comments = average_comments) %>%\n",
    "                     mutate(facebook_filtered_na, average_shares = average_shares) %>%\n",
    "                     mutate(facebook_filtered_na, average_total_interactions = average_total_interactions) %>%\n",
    "                     mutate(facebook_filtered_na, missing_rows = missing_rows) %>%\n",
    "                     select(observation_total, variable_total, average_likes, average_comments, average_shares, average_total_interactions,\n",
    "                            missing_rows) %>%\n",
    "                     slice(1)\n",
    "exploratory_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 4: Exploratory Data Analysis Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results of our exploratory table, we can notice a few things. First off, there are 500 (MAY CHANGE) observations in total for our training dataset alone, which means that theres a lot of data to process for our K-NN regression algorithm model, which will mean that regardless of the amount of folds we specify for the cross validation, our accuracy for our model will not be perfect. Secondly, it will be important to keep in mind the average of the total interactions (175) eventually, when analyzing our predictor visualization. \n",
    "\n",
    "We choose to use total_interactions as our predictor because we are interested in the number of literal interactions (the number of facebook users that like, share, or comment) with a post instead of lifetime impressions (the number of facebook users that saw the post). Moreover, we chose to use total_interactions, which is the sum of the number of 3 variables (like, share, and comment), as opposed to using the 3 variables as individual predictors because we only concerned about the overall number of interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a quick visualization of our dataset of our predictor and target variables (total interactions and hour of week) through the function ggpairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_ggpairs2 <- facebook_filtered_interactions %>%\n",
    "    select(hour_of_week, total_interactions) %>% \n",
    "    ggpairs()\n",
    "facebook_ggpairs2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure  5: Ggpairs Dataset between Hour_of_Week and Total_Interactions  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Best Time to Post \n",
    "Looking at the relationship between Hour of Week and Total Interactions, there seems to have no correlation between the two variables. This means as that as the week progresses, this does not affect the number of total interactions, and vice versa. While this may serve as an intial prediction, explore more affecing our results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods and Results: Predicting Highest Engagement based on Hour of Week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are clearly predicting a numerical value, this is a regression problem, predicting the highest engagement based on the hour of the week. As with any regression or classification model at the start, we'll split the data into training and testing data, setting aside 75 percent for the training data set and 25 percent with our testing data set, and setting our strata to be our target variable, which is total interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into training (75 percent of data set) and test data\n",
    "set.seed(2021)\n",
    "facebook_split <- initial_split(facebook_filtered_na, prop = 0.75, strata = total_interactions)\n",
    "facebook_train <- training(facebook_split)\n",
    "facebook_test <- testing(facebook_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin finding the most optimal K for our regression model. First we'll build a recipe and model specification, setting aside the variables required for our analysis, and as a good measure scaling our predictors for our recipe. We'll also set the neighbors to \"tune\" and mode to \"regression\" for our model specification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(2021)\n",
    "#Recipe for Post Hour\n",
    "facebook_recipe <- recipe(total_interactions ~ hour_of_week, data = facebook_train) %>%\n",
    "                   step_scale(all_predictors()) %>%\n",
    "                   step_center(all_predictors())\n",
    "#Model\n",
    "facebook_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) %>%\n",
    "                 set_engine(\"kknn\") %>%\n",
    "                 set_mode(\"regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's use cross-validation for five folds, and begin creating our workflow by adding our recipe and model. Next, we'll want to test 200 values of K by creating a tibble containing a sequence of 200 rows of K. Finally, we'll tune our workflow with the 200 values of K from gridivals, and make sure it resamples in our cross-validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(2021)\n",
    "#5 fold cross validation\n",
    "facebook_vfold <- vfold_cv(facebook_train, v = 5, strata = total_interactions)\n",
    "\n",
    "#workflow\n",
    "facebook_workflow <- workflow() %>%\n",
    "                     add_recipe(facebook_recipe) %>%\n",
    "                     add_model(facebook_spec)\n",
    "\n",
    "#testing 200 values to find the best value for K\n",
    "gridvals <- tibble(neighbors = seq(1, 200))\n",
    "\n",
    "#tuning workflow and resampling cross validation data set\n",
    "facebook_results <- facebook_workflow %>%\n",
    "                        tune_grid(resamples = facebook_vfold, grid = gridvals) %>%\n",
    "                        collect_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll filter for our min RMSE to help us find our most optimal K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Min amount of RMSPE along with mean and standard error to find the best K value to use\n",
    "facebook_min <- facebook_results %>%\n",
    "    filter(.metric == \"rmse\") %>%\n",
    "    arrange(mean)  %>% \n",
    "    slice(1)\n",
    "\n",
    "facebook_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 6: Min Number of Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods & Results: Retraining for Most Optimal K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our most optimal K, lets finally repeat the steps earlier, this time specifying K as 106. Now that we know the optimal K value is 68, we can use 68 nearest neighbours and refit our knn regression model with test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Min RMSPE\n",
    "set.seed(2021)\n",
    "k_min <- facebook_min %>%\n",
    "            pull(neighbors)\n",
    "\n",
    "#Using the best K in our model\n",
    "facebook_best_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = k_min) %>%\n",
    "                            set_engine(\"kknn\") %>%\n",
    "                            set_mode(\"regression\")\n",
    "\n",
    "#Reworking the workflow to incorporate our chosen K\n",
    "facebook_best_fit <- workflow() %>%\n",
    "                        add_recipe(facebook_recipe) %>%\n",
    "                        add_model(facebook_best_spec) %>%\n",
    "                        fit(data = facebook_train)\n",
    "\n",
    "#Summary of the training vs testing data\n",
    "facebook_summary <- facebook_best_fit %>% \n",
    "                       predict(facebook_test) %>%\n",
    "                       bind_cols(facebook_test) %>%\n",
    "                       metrics(truth = total_interactions, estimate = .pred)  \n",
    "facebook_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set.seed(2021)\n",
    "facebook_preds <-  facebook_best_fit %>%\n",
    "                   predict(facebook_test) %>%\n",
    "                   bind_cols(facebook_test)\n",
    "facebook_plot_final <- ggplot(facebook_preds, aes(x = hour_of_week, y = total_interactions)) +\n",
    "                       geom_point(alpha = 0.4) +\n",
    "                       xlab(\"Hour of Week\") +\n",
    "                       ylab(\"Total Interactions (Sum of Likes, Comments, and Shares)\") +\n",
    "                       geom_line(data = facebook_preds, aes(x = hour_of_week, y = .pred), color = \"blue\") +\n",
    "                       ggtitle(paste0(\"K = \", k_min))\n",
    "facebook_plot_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 7: Final Graph between Total Interactions and Hour of Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that converts post_day (weekday as a number from 1 to 7) to name of day\n",
    "weekday_no_to_name <- function(day_num) {\n",
    "    if (day_num == 1)\n",
    "        day <- \"Sunday\"\n",
    "    else if (day_num == 2)\n",
    "        day <- \"Monday\"\n",
    "    else if (day_num == 3)\n",
    "        day <- \"Tuesday\"\n",
    "    else if (day_num == 4)\n",
    "        day <- \"Wednesday\"\n",
    "    else if (day_num == 5)\n",
    "        day <- \"Thursday\"\n",
    "    else if (day_num == 6)\n",
    "        day <- \"Friday\"\n",
    "    else if (day_num == 7)\n",
    "        day <- \"Saturday\"\n",
    "    else\n",
    "        day <- \"Invalid\"\n",
    "    return(day)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show post hours, weekdays with highest post stats\n",
    "facebook_best <- facebook_preds %>% \n",
    "    select(.pred, post_day, post_hour, hour_of_week) %>%\n",
    "    group_by(hour_of_week, post_day, post_hour) %>%\n",
    "    summarize(predicted_interactions = mean(.pred)) %>% # same hour and weekday have same prediction; used summarize to remove duplicates\n",
    "    mutate(post_day = map(post_day, weekday_no_to_name)) %>% # convert weekday numbers to weekday names\n",
    "    arrange(desc(predicted_interactions))\n",
    "head(facebook_best)\n",
    "\n",
    "facebook_best_hours <- facebook_preds %>% \n",
    "    select(.pred, post_day, post_hour, hour_of_week) %>%\n",
    "    group_by(post_hour) %>%\n",
    "    summarize(mean_predicted_interactions = mean(.pred)) %>% \n",
    "    arrange(desc(mean_predicted_interactions))\n",
    "head(facebook_best_hours)\n",
    "\n",
    "facebook_best_days <- facebook_preds %>% \n",
    "    select(.pred, post_day, post_hour, hour_of_week) %>%\n",
    "    group_by(post_day) %>%\n",
    "    summarize(mean_predicted_interactions = mean(.pred)) %>% \n",
    "    arrange(desc(mean_predicted_interactions)) %>% \n",
    "    mutate(post_day = map(post_day, weekday_no_to_name)) # convert weekday numbers to weekday names\n",
    "head(facebook_best_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods & Results: Linear Regression Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN Regression isn't the only method for prediction quantatative values. A more common and popular method is Linear Regression, which instead of looking at the K-nearest neighbours and averaging over their values for a prediction, we use all the training data points to create a straight line of best fit. This line is then used to \"look up\" the predicted value. This different prediction method may give us more accurate results, so we will be creating a linear regression model for further analysis and comparison to our KNN model later on in the report.\n",
    "\n",
    "Using our training data from before, we start by creating a new model specification that sets the engine to \"lm\" and the mode to \"regression\" (like before) and a new recipe. Unlike the KNN Facebook recipe, we do not need to scale and center the data for linear regression as standardization does not affect the fit of the model at all. It does impact the coefficients in the linear slope equation though, but its usually easier to interpret the best fit coefficients if we don't standardize so we're choosing not to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_spec <- linear_reg() %>%\n",
    "    set_engine(\"lm\") %>%\n",
    "    set_mode(\"regression\")\n",
    "\n",
    "fb_recipe <- recipe(total_interactions ~ hour_of_week, data = facebook_train)\n",
    "\n",
    "fb_fit <- workflow() %>%\n",
    "    add_recipe(fb_recipe) %>%\n",
    "    add_model(fb_spec) %>%\n",
    "    fit(data = facebook_train)\n",
    "fb_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods & Results: Slope Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the given model, we see that our coefficients are (intercept) β0 = 175.02958, and (slope) β1 = -0.00181. This gives us a slope equation of: <br>\n",
    "<b>Total Interactions = 175.02958 - 0.00181 * (hour_of_week)</b><br>\n",
    "The model is predicting that interactions roughly start at 180.85, and every hour_of_week decreases total interactions by 0.00181. This fits our exploratory analysis earlier, where we initially discovered that there is no correlation between hour_of_week and total interactions.<br><br><br>\n",
    "Continuing on, we compute our test results by using our model to predict the values in our test set, and then we bind the predictions into the test set as its own column. This allows us to compute the accuracy by comparing each instance where the model got the prediction right or wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods & Results: Model Accuracy Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_test_results <- fb_fit %>%\n",
    "    predict(facebook_test) %>%\n",
    "    bind_cols(facebook_test) %>%\n",
    "    metrics(truth = total_interactions, estimate = .pred)\n",
    "\n",
    "facebook_linear_preds <- fb_fit %>%\n",
    "                   predict(facebook_test) %>%\n",
    "                   bind_cols(facebook_test)\n",
    "\n",
    "facebook_linear_pred_model <- facebook_linear_preds %>%\n",
    "                              select(.pred, total_interactions)\n",
    "facebook_linear_pred_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We selected only the relevant data for the accuracy test, and from what we see from the previewed observations, it seems that the model is not accurate at all. This fits our exploratory analysis that the correlation is minimal / non-existant. <br> <br>\n",
    "The next and final step is to graph the relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_plot_final <- ggplot(facebook_linear_preds, aes(x = hour_of_week, y = total_interactions)) +\n",
    "                       geom_point(alpha = 0.1) +\n",
    "                       xlab(\"Hour of week\") +\n",
    "                       ylab(\"Total Interactions (Sum of Likes, Comments, and Shares)\") +\n",
    "                       geom_line(data = facebook_linear_preds, aes(x = hour_of_week, y = .pred), color = \"blue\") +\n",
    "                       ggtitle(\"The Predicted Relationship between \n",
    "Total Interactions and the Hour of Week\") +\n",
    "                       theme(text = element_text(size = 16)) \n",
    "facebook_plot_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization of the model affirms what our model was predicting earlier; the correlation between Total Interactions and Hour of Week is minimal. The slope of the line is nearly zero, indicating that the relationship is very weak, but it is still a weak negative slope. This does show that there is a relationship between the two variables, but this relationship serves no purpose for predictive analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
